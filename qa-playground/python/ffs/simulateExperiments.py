#!/usr/bin/env python
import sys
import requests
import json
import random
from string import letters
import time
import datetime
import argparse
import ast
from random_words import RandomWords
parser = argparse.ArgumentParser()
'''
This script is created to help generate predictable test data for SI experiment simulation so we can help test bulls eye reporting around
experiment conversion metrics. It is best effort attempt, not accurate since we don't have total control of our users and which exp group they belong to. 
In General the experiment simulation attempts to convert: 
at most 50% of non-zero hits into a CLICK-THROUGH CONVERSION
at most 40% of non-zero hits into a HARD CONVERSION for VIEWS
at most 10% of non-zero hits into a PURCHASE EVENTS

Using external experiments_controlls.txt the script can define view_portion behaviors for certain titles and define search terms
for experiments that promotes titles based on user typed terms.

The output json file which records actual events generated by this simulation should be used
to compare the data and charts SI generates by the ETL jobs - not accurate but general enough to detect inconsistencies.

This runs only against si-ec2 instance for now:
example: python simulateExperiment 25000 50 prefix --useCase QATest-SI2327 --popName tivo-nlu --exp_properties experiment_controlls.txt --output ExperimentSI-SI2327 --test_desc "Some description about this experiment that helps to tie back to actual created MVP in SI"
'''

def parseArgs(parser):
    parser = argparse.ArgumentParser()
    parser.add_argument("totalQueries", help="number of queries to run the experiment for",type=int)
    parser.add_argument("totalUniqueUsers", help="number of unique users to generate for this experiment",type=int)
    parser.add_argument("searchType", help="prefix, search, auto_suggest, predictive - defaults to SEARCH")
    parser.add_argument("--popName", help="give a population name to determine where to query and posts to -- defaults to qa-offers")
    parser.add_argument("--overrideTerms", help="generate random search terms for experiment, default to false")
    parser.add_argument("--randomTermsCount", help="how many different terms to generate, default to 10")
    parser.add_argument("--useCase", help="Use Case for Audience Measurement Defaults to QATesting, set param to NA if do not want to use this")
    parser.add_argument("--randomize", help="whether to randomize title selection from search results defaults to false -- always choose first group -- set to false group!enabled")
    parser.add_argument("--exp_properties", help="list movie from external file and other properties help verify experiment results")
    parser.add_argument("--search_terms", help="Quote enclosed | separated terms to vary search")
    parser.add_argument("--output", help="Write summary to output file -- defaults to SIExperiment-{epoch}.json")
    parser.add_argument("--test_desc", help="Optional description of scenarios and test being simulated")
    args = parser.parse_args()
    return args

givenArgs = parseArgs(parser)
totalQueries = givenArgs.totalQueries
userCount = givenArgs.totalUniqueUsers
searchType = givenArgs.searchType
useCase = "&__useCase=QATesting" if givenArgs.useCase is None else "&__useCase="+givenArgs.useCase
useCase = "" if "useCase=NA" in useCase else useCase  #allow for option to not use useCase at all when value param passed is "NA"
popName = "qa-offers" if givenArgs.popName is None else givenArgs.popName
randomize = "t" in str(givenArgs.randomize).lower()
overrideTerms = "t" in str(givenArgs.overrideTerms).lower()
randomTermsCount = "10" if givenArgs.randomTermsCount is None else givenArgs.randomTermsCount
searchQs = ["show","The tonight","tonight","talk", "blood","lethal","blood","drive","team","blazing","shooter","spock","american","pass","factor","deux","survivor","ghost","city","rogue","arrival","star","passenger","moana","deadpool","captain","India","Cambodia","China","Nepal","North Carolina","California","Annihilation","Panther","Blockers","Lover","Dia","Double","First","Fox", "Isle of Dogs","Leave","Trace","Life", "Death","Final","Rider","Catcher","Adams","Jason Bourne","Jason","Bourne","Stranger Things", "The Simpsons"] #random search q fillers
searchQs = searchQs if  givenArgs.search_terms is None else givenArgs.search_terms.split("|")
scenario_description = "" if givenArgs.test_desc is None else givenArgs.test_desc
views_portion = {} # how much to view each title in percentage where 80% is considered complete
genres =["SciFi","Comedy","Action","Drama"]
segments=["Northeast","Southeast","Northwest", "Southwest"] #&segment=
products=["My Cool App","MyIOSPhone","MyAndroidPhone","XYZ Experience"] #&product=
deviceTypes=["IPAD","STB","XBOX"]   #&deviceType=
groupAlias = ["groups","hits"] #use groups only for predictive/watch now where grouping is enabled
channels=["114365","100708","100484","100709","100720"] #give arb. channel to ensure channel shows up in charts
ommit_queryids_titles=[]
defaultGroupings = "&__group=true&__group.by=releaseYear&__group.limit=5"
prop_path =""
variationIds=["39e846eb-31e4-4f6a-8fe4-a6a7078f4928","bg-39e846eb-31e4-4f6a-8fe4-a6a7078f4928","cg-39e846eb-31e4-4f6a-8fe4-a6a7078f4928"] #hard coded or replaced from exp_properties array
experimentTreatments= {} # tracks what users are given what experiment treatments
nonPromotedQueryCounts=0
promotedQueryCounts=0
#utcOffsets=["4","-4","-40","40","+40","04","-04","+04","0400","-0400","+0400","0040","-0040","0065","-0065","+0065",-66,66,30] for testing bad utc offsets
utcOffsets=["-0400","-0500","-0700"]
def getExperimentProperties(propPath):
    print("Search & experiment property is being read from file")
    global searchQs
    global views_portion
    global ommit_queryids_titles
    global prop_path
    global variationIds
    global experimentTreatments
    views_portion = {}
    with open(propPath) as p:
        for line in p:
            if("views_portion" in line):
                views_portion = ast.literal_eval(line[len("views_portion="):len(line.strip("\n"))])
            if("search_terms" in line):
                searchQs = ast.literal_eval(line[len("search_terms="):len(line.strip("\n"))])
            if("ommit_queryids_titles" in line):
                ommit_queryids_titles = ast.literal_eval(line[len("ommit_queryids_titles="):len(line.strip("\n"))])
            if("variationIds" in line):
                variationIds = ast.literal_eval(line[len("variationIds="):len(line.strip("\n"))])
                for v in variationIds:
                    experimentTreatments[str(v)]=[{"userId":0}] #initialize all the treatments
        prop_path = propPath
    p.close()
    return prop_path
#generate x number of random words to use for searching
def getRandomSearchWord(listSize):
    global searchQs
    rw = RandomWords()
    word = rw.random_word()
    searchQs=[] # clear existing word list
    for w in range(0,listSize):
        searchQs.append(rw.random_word())
    return searchQs
#load properties for experiment if a file is given -- otherwise use the defaults above
if givenArgs.exp_properties is not None: getExperimentProperties("../data/"+givenArgs.exp_properties)
campaignTreatments= {variationIds[0]:{'users':{}},variationIds[1]:{'users':{}},variationIds[2]:{'users':{}}} # tracks what users are given what experiment treatments
allActivatedRules = {''} # save all activated rules so we can verify that the experiments we wanted to test against did indeed got activated
activatedCounts = {}    # save how many time each experiment variation was activated
choiceDistribution={1:0,2:0,3:0} #Map to get distribution how many random choices of 1,2,or 3 were presented
totalPostDistribution={"totalSkips":0,"zeroHits":0,"hitAndPost":0,"totalQueries":0,"postedSegment":0,"postedProduct":0,"skipChoice":0,"IPAD":0,"IPHONE":0,"XBOX":0,"ROKU":0,"STB":0,"views":0,"clicks":0,"purchase_rental":0,"failed_posts":0}
channelDistribution={channels[0]:0,channels[1]:0,channels[2]:0,channels[3]:0,channels[4]:0}
segmentProductDistribution={segments[0]:0,segments[1]:0,segments[2]:0,segments[3]:0,
                            products[0]:0,products[1]:0,products[2]:0,products[3]:0}
postedTitles={} # keep track of how many differents titles were being posted so we can compare conversions results
totalPostViewsPerTitle={} #keep track of how times we post views for each title
totalPostedImplicitEvents={} #keep track of how times we click/like implicit event for each title
totalPostedPurchaseEvents={} #Keep track of how times we purchased each title

start = int(time.time())*1000
blacklist = set(['', u'AdultFilter', u'NewAdultFilter', u'MovieOrShow', u'GroupBy',u'EnableSynonyms',u'EnglishOnly',u'EnableGrouping',u'PrefixSearchEqualizer',u'GenreFilterWestern',
                 u'GenreFilterAction', u'BoostList',u'EpisodeFilter',u'GroupParams',u'AdultFilterTaps',u'GenreFilterThriller',u'PersonalizationBoost',u'GenreFilterDrama',u'PopularityBoost',
                 u'RemoveBlacklistedContent',u'GenreFilterComedy',u'PaidProgrammingFilter',u'GenreFilterSciFi'])
headers = {'Content-Type':'application/json'}
#for tivo-nlu
search_urls ={"qa-offers-watch":"http://localhost/sd/qa-offers/taps/watchNow?__limit=10&__offset=0&__wt=ffsjson&__schedule.limit=12&__fl=id,title&__lang=en",
              "qa-offers-search":"http://localhost:8081/sd/qa-offers/taps/search?&__fl=id,title&__utcOffset=-0400&__debugffs=merchandising,querylog&queryType=SEARCH",
              "qa-offers-prefix":"http://localhost:8081/sd/qa-offers/taps/prefixSearch?&__wt=ffsjson&__fl=id,title&__utcOffset=-0400&__debugffs=merchandising,querylog&queryType=PREFIX_SEARCH",
              "tivo-nlu-prefix":"http://localhost:8081/sd/tivo-nlu/taps/standard/prefixSearch?&__wt=ffsjson&__fl=id,title&__utcOffset=-0400&__debugffs=merchandising,querylog&queryType=PREFIX_SEARCH&__lang=en",
              "tivo-nlu-search":"http://localhost:8081/sd/tivo-nlu/taps/standard/search?__utcOffset=-0400&__debugffs=merchandising,querylog&queryType=SEARCH&__lang=en",
              "tivo-nlu-browse":"http://localhost:8081/sd/tivo-nlu/taps/standard/browse?lang=en&limit=5&offset=0&__utcOffset=-0400&__debugffs=merchandising,querylog&queryType=BROWSE"}

postUrls = {"qa-offers":'http://localhost/sd/qa-offers/viewEvents',
            "tivo-nlu":'http://localhost/sd/tivo-nlu/viewEvents',
            "tivo":'http://localhost/sd/tivo/viewEvents'}
implicitEventsPostURLs = {"qa-offers":'http://localhost/sd/qa-offers/implicitEvents',
                          "tivo-nlu":'http://localhost/sd/tivo-nlu/implicitEvents',
                          "tivo":'http://localhost/sd/tivo/implicitEvents'} #for VIEW_DETAILS (CLICK-THROUGHS)
purchaseEventsPostURLs = {"qa-offers":'http://localhost/sd/qa-offers/purchaseEvents',
                          "tivo-nlu":'http://localhost/sd/tivo-nlu/purchaseEvents',
                          "tivo":'http://localhost/sd/tivo/purchaseEvents'} #for purchase

url=search_urls[popName+"-"+searchType]

# set the correct urls for POST events url
postUrl=postUrls[popName]
implicitEventsPostURL=implicitEventsPostURLs[popName]
purchaseEventsPostURL=purchaseEventsPostURLs[popName]

print("Starting Events Generation on population: "+popName)
print("Search Type: "+searchType)
print("Search URLS: "+url)
print("PostEvents URL:  "+postUrl)
print("ImplicitEvent URL:  "+implicitEventsPostURL)
print("PurchaseEvents URL:  "+purchaseEventsPostURL+"\n")

data = [{
    'itemId':'fakeItemId',
    'userId':'userx',
    'startTime':1475812800000,
    'endTime': 1475812900000,
    'queryId':'fakequeryId',
    'product':'mobile',
    'segment':'East',
    'utcOffset':'-0400',
    'channel': channels[0]
}
]
#click throughs
view_details = [
    {
        "type": "VIEW_DETAILS",
        "itemId": "abcd-1234",
        "userId": "12345",
        "deviceType": "iPad",
        "timestamp": "NOW",
        "segment": "Southeast",
        "product": "My Mobile App",
        "queryId": "6c48898dcf2c3a1d"
    }
]
#purchases
purchase_event = [ {
    "type": "RENTAL",
    "itemId": "abcd-1234",
    "userId": "12345",
    "deviceType": "iPad",
    "retailPrice": 2.99,
    "purchasePrice": 2.99,
    "timestamp": "NOW",
    "segment": "Southeast",
    "product": "My Mobile App",
    "queryId": "6c48898dcf2c3a1d"
}
]

#generate a now time 15 minutes ago to 15mins from now + x randomMinuteSpan i.e 15 + random.nextInt(0,5) for 30+(0,5) minutes
def generateNOWTime(randMinuteSpan):
    nowTime = (int(time.time())*1000)-900000 #set it to 15 minutes ago
    nowTimePlusRandSpan = nowTime + 900000 + random.randint(0, randMinuteSpan*60000)
    NOW_MAP = {'nowTime':nowTime, 'nowTimePlusRandSpan':nowTimePlusRandSpan}
    return NOW_MAP
    #duration in seconds somewhere between complete and incomplete ( 75%<=viewDuration<=85%) where 100% = 60 minutes  | SI considers 80% views as complete according McCarther
def generateViewDuration():
    viewDuration = 2700+random.randint(0,360) #between 45 minute and 51 minutes or  ( 75%<=viewDuration<=85%) of 60 minutes
    return viewDuration
def pickRandomChannel():
    return channels[random.randint(0,len(channels)-1)]
def pickRandomProduct():
    return products[random.randint(0,len(products)-1)]
def pickRandomSegment():
    return segments[random.randint(0,len(segments)-1)]

    #return queryId, randomTime, titleId
def parseQueryResults(response):
    res = response.json()
    grouping = groupAlias[0]  #0 -groups, 1- hits: use groups only for predictive/watch now or where grouping is enabled
    queryId=str(res['queryId'])
    groupingEnabled = ("groups" in res) and ("hits" not in res)
    randomTitleId="",
    randomPick="";
    randomTitle="";
    viewAbleLimit = len(res['hits'])
    if(viewAbleLimit<=5): randomPick=random.randint(0,(viewAbleLimit-1)) # pick some random title within a reasonable limit param (5)
    if(randomize!=True):
        if(randomPick==""): randomPick=random.randint(0,viewAbleLimit-1)
        randomTitlePick = randomPick # 0get the first title (likely our boost)
        randomTitleId=(res['hits'][randomTitlePick]['id']).encode('utf-8').strip()
        if("tivo" in popName):
            randomTitle=(res['hits'][randomTitlePick]['title']["en"]).encode('utf-8').strip() #only works with tivo-nlu
        else:
            randomTitle=(res['hits'][randomTitlePick]['metadata']["title"]).encode('utf-8').strip() #only works with non-standard catalog cores
        randomPick = str(randomTitle)+" - "+str(randomTitleId)
        parsedResults = {"itemId":str(randomTitleId),'queryId': queryId, "randomPick":randomPick,"randomTitle":randomTitle }
        return parsedResults
    else: #randomly select a group
        randomTitlePick = random.randint(0,len(res[grouping])-1) #get some other random returned title group
        print ("\tRandom title position picked "+str(randomTitlePick))
        print ("\tRandom title Id "+(res['hits'][randomTitlePick]['id']).encode('utf-8').strip())
        print ("\tQueryId "+str(queryId))

    if("prefix" in searchType and not groupingEnabled):
        grouping=groupAlias[1]
        print("\tdon't use groups --use "+grouping+ " instead")
        randomTitle = (res['hits'][randomTitlePick]['metadata']['title']).encode('utf-8').strip()
        randomTitleId = (res['hits'][randomTitlePick]['id']).encode('utf-8').strip()
    elif ("prefix" in searchType):
        #print("\tWe have groupings enabled")
        randomTitle = (res[grouping][randomTitlePick]['hits'][0]['metadata']['title']).encode('utf-8').strip()
        randomTitleId = (res[grouping][randomTitlePick]['hits'][0]['id']).encode('utf-8').strip()
    randomPick = str(randomTitle)+" - "+str(randomTitleId)
    parsedResults = {"itemId":str(randomTitleId),'queryId': queryId, "randomPick":randomPick,"randomTitle":randomTitle }
    return parsedResults

def recordPostDistributions(deviceType,channel,segment,product,results,type):
    totalPostDistribution[type]+=1
    totalPostDistribution["hitAndPost"]+=1
    totalPostDistribution["postedProduct"]+=1
    totalPostDistribution["totalQueries"]+=1
    totalPostDistribution["postedSegment"]+=1
    totalPostDistribution[deviceType]+=1
    if(channel!=None):
        channelDistribution[channel]+=1
    segmentProductDistribution[segment]+=1
    segmentProductDistribution[product]+=1
    if(results["randomTitle"] not in postedTitles): postedTitles[results["randomTitle"]]=1
    else: postedTitles[results["randomTitle"]]+=1
    print("\tRECORDING POST FOR TYPE "+str(type).upper()+"\t\n"+str(totalPostDistribution)+" - "+results["randomTitle"]+"\n")

    #record different type of posts event for each title
    if(type=="views"):
        if(results["randomTitle"] not in totalPostViewsPerTitle): totalPostViewsPerTitle[results["randomTitle"]]=1
        else: totalPostViewsPerTitle[results["randomTitle"]]+=1
    if(type=="clicks"):
        if(results["randomTitle"] not in totalPostedImplicitEvents): totalPostedImplicitEvents[results["randomTitle"]]=1
        else: totalPostedImplicitEvents[results["randomTitle"]]+=1
    if(type=="purchase_rental"):
        if(results["randomTitle"] not in totalPostedPurchaseEvents): totalPostedPurchaseEvents[results["randomTitle"]]=1
        else: totalPostedPurchaseEvents[results["randomTitle"]]+=1


def postViewsForTopResults(response,userId,deviceType,product,segment,searchQ):
    results = parseQueryResults(response)
    NOW_MAP = generateNOWTime(5)
    nowTime = NOW_MAP['nowTime']
    endTime = NOW_MAP['nowTimePlusRandSpan']
    viewDuration = generateViewDuration()
    channel = pickRandomChannel()
    queryId = results["queryId"]
    #experiment-specific logic set by our properties file
    # Go through each title in config and check to see if they matched the randomTitle picked and give it the view duration set in our property
    # This helps us verify complete views where anything above 80 is considered complete by SI (SI-2327)
    for t in views_portion:
        if t in results["randomTitle"]:
            viewDuration = int(((float(views_portion[t]))/100)*3600)
            endTime = (viewDuration*1000)+nowTime   # make sure the offset difference also matched the viewDuration set
            print ("Found a title that matched our property and setting the duration accordingly "+str(viewDuration))
            break;

    #experiment-specific log set by our properties file
    #Go through each title and find one requested to ommit queryIds as part of its viewsPost and removeQuerId
    for o in ommit_queryids_titles:
        if o in results["randomTitle"]:
            queryId = None
            print("Ommit queryId from this title -- it should show up as No Data Provided Under Use Case - SI-2399")
            break;
    data[0]['utcOffset']=utcOffsets[random.randint(0,len(utcOffsets)-1)] #pick a random utcOffset
    data[0]['userId']=str(userId)
    data[0]['itemId']= results["itemId"]    #str(randomTitleId)
    data[0]['startTime']=nowTime
    data[0]['endTime']=endTime
    data[0]['queryId']= queryId  #queryId
    data[0]['product']=product #pick a random product
    data[0]['segment']=segment #pick a random segment
    data[0]['channel']=channel
    data[0]['deviceType']=deviceType
    data[0]['contentDuration']=3600 # 1 hour
    data[0]['viewDuration'] = viewDuration # 80 percent of a hour for si to consider a complete view
    #print "Posting %s's views at %dGMT for: \n" %(userId,nowTime)+"\n"+results["randomPick"]
    post =requests.post(postUrl,headers=headers,data=json.dumps(data))
    if post.status_code==202:
        recordPostDistributions(deviceType,channel,segment,product,results,"views")
    else:
        print "failed views posting for %s with status code %d" %(userId,post.status_code)+"data: \n"+str(data)
        totalPostDistribution["totalQueries"]+=1 #even if we didn't post successfully, we still queried
        totalPostDistribution["failed_posts"]+=1 #record this failed event
    return post.status_code

def postImplicitEventsForClicks(response,userId,deviceType,product,segment,searchQ):
    results = parseQueryResults(response)
    nowTime = generateNOWTime(5)['nowTime']
    view_details[0]["type"]="VIEW_DETAILS"
    view_details[0]["itemId"] = results["itemId"]
    view_details[0]["userId"] = userId
    view_details[0]["deviceType"] = deviceType
    view_details[0]["timestamp"]=nowTime
    view_details[0]["segment"]=segment
    view_details[0]["product"]=product
    view_details[0]["queryId"]=results["queryId"]
    #print "Posting %s's CLICK-THROUGH VIEW_DETAISL at %dGMT for: \n" %(userId,nowTime)+"\n"+results["randomPick"]
    #print("\t\t\n\n"+str(view_details)+"\n\n")

    post = requests.post(implicitEventsPostURL,headers=headers,data=json.dumps(view_details))
    if  post.status_code==202:
        recordPostDistributions(deviceType,None,segment,product,results,"clicks")
    else:
        print "failed IMPLICIT views VIEW_DETAILS posting for %s with status code %d" %(userId,post.status_code)+"view_details: \n"+str(view_details)
        totalPostDistribution["totalQueries"]+=1 #even if we didn't post successfully, we still queried
        totalPostDistribution["failed_posts"]+=1 #record this failed event
    return post.status_code

def postPurchaseEvents(response,userId,deviceType,product,segment,searchQ):
    results = parseQueryResults(response)
    nowTime = generateNOWTime(5)['nowTime']
    purchase_event[0]["type"]="RENTAL"
    purchase_event[0]["itemId"]=results["itemId"]
    purchase_event[0]["userId"]=userId
    purchase_event[0]["retailPrice"]=2.99
    purchase_event[0]["purchasePrice"]=2.99
    purchase_event[0]["timestamp"]=nowTime
    purchase_event[0]["segment"]=segment
    purchase_event[0]["product"]=product
    purchase_event[0]["queryId"]=results["queryId"]
    purchase_event[0]["deviceType"]=deviceType
    #print "Posting %s's Purchased Events at %dGMT for: \n" %(userId,nowTime)+"\n"+results["randomPick"]
    #print("\t\t\n\n"+str(purchase_event)+"\n\n")
    post = requests.post(purchaseEventsPostURL,headers=headers,data=json.dumps(purchase_event))
    if  post.status_code==202:
        recordPostDistributions(deviceType,None,segment,product,results,"purchase_rental")
    else:
        print "failed Purchase Events posting for %s with status code %d" %(userId,post.status_code)+"purchase_event_rental: \n"+str(purchase_event)
        totalPostDistribution["totalQueries"]+=1 #even if we didn't post successfully, we still queried
        totalPostDistribution["failed_posts"]+=1 #record this failed event
    return post.status_code

    #what groups were sent to what treatment? This may be some value later
    #WIP doesn't do what is intended for now (works for pre-9.0 where experiments were shown under merchandising
    #No longer work now as experiments are shown under queryEvents >> experimentActivations
def trackTreatments(userId,variationId):
    global experimentTreatments
    global variationIds
    if(str(variationId) in experimentTreatments):
        currentUserIndex = None
        users_set = {''}
        for x in range(len(experimentTreatments[str(variationId)])):
            profile = experimentTreatments[str(variationId)][x]
            for name in profile:
                users_set.add(name)
            if(userId in users_set):
                currentUserIndex = x
                break; #found it!
        #is the current user already in the set? if not add it
        if(userId not in users_set):
            experimentTreatments[str(variationId)].append({userId:1})
        #else we increment the count for that user
        else:
            experimentTreatments[str(variationId)][currentUserIndex][userId]+=1 # increment the count for that user
    return experimentTreatments

#
def trackCampaignsTreatment(userId,variationId):
    global campaignTreatments
    global variationIds
    if(str(variationId) in campaignTreatments):
        currentUserIndex = None
        users_set = {''}
        for x in range(len(campaignTreatments[str(variationId)])):
            profile = campaignTreatments[str(variationId)][x]
            for name in profile:
                users_set.add(name)
            if(userId in users_set):
                currentUserIndex = x
                break; #found it!
        #is the current user already in the set? if not add it
        if(userId not in users_set):
            campaignTreatments[str(variationId)].append({userId:1})
        #else we increment the count for that user
        else:
            campaignTreatments[str(variationId)][currentUserIndex][userId]+=1 # increment the count for that user
    return campaignTreatments


#overload the search words loaded from property files or given at run time
if overrideTerms:
    getRandomSearchWord(int(randomTermsCount))#ignore searchQS given or loaded from property in favor of RandomWords module
    print("Getting random "+str(randomTermsCount)+" search terms")
    print "\n"
#send queries for users (guarantees at least #unique users w/  userCount)
#convert at most 50% of non-zero hits into a CLICK-THROUGH CONVERSION
#convert at most 40% of non-zero hits into a HARD CONVERSION for VIEWS
#convert at most 10% of non-zero hits into a PURCHASE EVENTS
def runSimulation():
    count = 0
    print("the variationsIds are "+str(variationIds))
    print("campaign treatments are "+str(campaignTreatments))
    u = 0;
    global nonPromotedQueryCounts
    global promotedQueryCounts
    for n in range(int(totalQueries)):
        searchQ = random.choice(searchQs)
        u+=1 # give a higher chance same random users are generated
        if(u%userCount==0): u=0 #by resetting user number to zero every 500th posts
        product=pickRandomProduct()
        segment=pickRandomSegment()
        if("prefix" in searchType): url=search_urls[popName+"-"+searchType] #use prefixSearch tap
        elif("search" in searchType): url=search_urls[popName+"-"+searchType]
        elif("browse" in searchType): url=search_urls[popName+"-"+searchType]
        else:   url=search_urls[popName] #defaults to WatchNow Tap
        user='user-'+str(random.choice(letters))+str(u)
        deviceType=deviceTypes[random.randint(0,len(deviceTypes)-1)]
        if("browse" not in searchType):
            request = url+"&userId="+user+"&genreFilter="+genres[random.randint(0,len(genres)-1)]+"&q=\""+searchQ+"\"&segment="+segment+"&product="+product+"&deviceType="+deviceType+useCase+"&__fq=objectType:* -objectType:(Station | Program | SportsTeam | Genre)&__fq=contentType:* -contentType:(Station | Program | SportsTeam | Genre)"
        else:
            request = url+"&userId="+user+"&genreFilter="+genres[random.randint(0,len(genres)-1)]+"&__fq=title:\""+searchQ+"\"&segment="+segment+"&product="+product+"&deviceType="+deviceType+useCase+"&__fq=objectType:* -objectType:(Station | Program | SportsTeam | Genre)&__fq=contentType:* -contentType:(Station | Program | SportsTeam | Genre)"
        if randomize:
            request=request+defaultGroupings
        r = requests.get(request, auth=('fake_admin','fake_admin'))
        print request
        if(r.status_code!=200):
            continue; #keep going -- a 400 because of some invalid bad values like unaccepted segment or product value
        res = r.json()
        activatedRules = res['debug']['merchandising']['rulesActivated']

        if(activatedRules is not None):#ignore if no rules were found
            for rule in activatedRules:
                allActivatedRules.add(rule)
                if rule in activatedCounts:
                    activatedCounts[rule]+=1
                else:
                    activatedCounts[rule]=1
                trackTreatments(user,rule) # only for pre-9.0 experiments. Might be useful again once experiments are added back to ffs
        try:
            activatedExperiments = res['debug']['queryEvent']['experimentActivations'] # experiment activations now diff. place in 9.x
            if(activatedExperiments is not None):
                promotedQueryCounts+=1
                print("\nQuery Activated Experiment: \""+str(activatedExperiments[0]['experimentName'])+"\" - "+str(activatedExperiments[0]['variationId'])+" for user: "+user)
                activatedExp = activatedExperiments[0]['variationId']
                #only care about the experiment we set up -- so track them
                if activatedExp in campaignTreatments:
                    #great now is the user in that experiment already?
                    if user not in campaignTreatments[activatedExp]['users']:
                        campaignTreatments[activatedExp]['users'][user]=1 #add the user to the experiment group for the first time
                    else:
                        campaignTreatments[activatedExp]['users'][user]=campaignTreatments[activatedExp]['users'][user]+1 #increment the user's count to that experiment group instead
        except KeyError:
            print("\n This Search Did Not Trigger Campaign Promotion \n")
            nonPromotedQueryCounts+=1
        if(res['hitCount'])==0:
            totalPostDistribution["zeroHits"]+=1 #add so we can see how many totally got zero
            totalPostDistribution["totalQueries"]+=1
            totalPostDistribution["totalSkips"]+=1; #this add both skip from choice and skip by zero hits
            print ("Zero hits - Nothing to post\n"+str(totalPostDistribution)+"\n")
            continue;
        if(r.status_code==200): count+=1
        #choice=random.choice(['1','2','3'])
        # if choice=='1' or choice=='3': -- let's use mod instead of randomization of choices to distribute the user event types
        if n%2==0:
            #Turning the 50percent of queries into clicks or VIEW_DETAILS
            postImplicitEventsForClicks(r,user,deviceType,product,segment,searchQ)
        elif n%5!=0: #skip every 5th value means 40% of queries remain unposted -- turn these into views or hard conversion
            #Turning remaining 40percent of queries into clicks or views for hard conversion: the query
            postViewsForTopResults(r,user,deviceType,product,segment,searchQ)
        else:  #The remaining 10% turn them all into purchases
            #Turning remaining 10percent of queries into purchase the query
            postPurchaseEvents(r,user,deviceType,product,segment,searchQ)


def jsonifyDictionar(dictionary):
    return json.dumps(dictionary)

    #find out if there are in users belonging into more than one group -- that would be a bug
def findDuplicateTreatmentUsers(treatments):
    record = {}
    compared={}
    dupset={}
    dups = {}
    for t in treatments:
        accounts = treatments[t]
        t_users={''}
        for user in accounts:
            for name,count in user.items():
                t_users.add(name)
                record[t]=t_users
    #let's check for duplicates or intersections between sets
    for t in treatments:
        for u in treatments:
            if(u==t) or u in compared: continue; # no need to compare to itself or something already compared with
            #print("Duplicate Checks FOR "+t+" and "+u+"?" +str(record[t] & record[u]))
            dupset = record[t] & record[u]
            if(len(dupset)>2):
                dups[t+"-AND-"+u]=[dupset] #record the duplicate
        compared[t]=False

    if(len(dups)>=1):
        print("there are some duplicates in group "+str(dups))
    return dups

#summarize the experiment statistics
#percent of queries that were turned into hard conversion (VIEWS)
#percent of queries that were turned into click throughs (VIEW_DETAILS)
#percent of queries that were turned into purchases (RENTALS)
#percent of zero hits queries
#total number of queries
#start time - endTime | duration of experiment
#average view durations

def printStatistics(allActivatedRules):
    end = int(time.time())*1000
    runDuration = end-start
    start_epoch = start/1000
    end_epoch = end/1000
    s_epoch = datetime.datetime.fromtimestamp(start_epoch).strftime('%Y-%m-%d %H:%M:%S')
    e_epoch =  datetime.datetime.fromtimestamp(end_epoch).strftime('%Y-%m-%d %H:%M:%S')
    minutes = (0.0166667*.001)*runDuration
    totalQueries = float(totalPostDistribution["totalQueries"])
    totalPosted = float(totalPostDistribution["hitAndPost"])
    zeroHitQueries = float(totalPostDistribution["zeroHits"])
    viewsConversion = float(totalPostDistribution["views"])
    clicksConversion = float(totalPostDistribution["clicks"])
    purchaseConversion = float(totalPostDistribution["purchase_rental"])
    target_out = "SIExperiment-"+str(end)+".json" if givenArgs.output is None else givenArgs.output+"_"+str(end)+".json"
    f = open(target_out,'w')

    #f.write("{\"totalQueries\":10}")
    f.write("{") # open for json object
    f.write("\"totalQueries\":"+str(totalQueries)+",")
    f.write("\"uniqueUserEstimate\":"+str(userCount)+",")
    #f.write("\"experiment_description\":\""+str(scenario_description)+"\",")
    f.write("\"experiment_description\":"+jsonifyDictionar(scenario_description)+",")
    f.write("\"results_summary\":[") #opening for results_summary array object
    f.write("{\"startTime\":\""+str(s_epoch)+"\"},")
    f.write("{\"endTime\":\""+str(e_epoch)+"\"},")
    f.write("{\"duration\":\""+str(minutes)+" Minutes\"},")

    f.write("{\"Total_From_Posted\":[") #opening for summary-->total_posted
    f.write("{\"Total_Queries\":"+str(int(totalQueries))+"},")
    f.write("{\"Total_Posted\":\""+str(totalPosted)+" - "+str(int(totalPosted/totalPosted*100))+"%\"},")
    f.write("{\"Views\":\""+str(viewsConversion)+" - "+str(int(viewsConversion/totalPosted*100))+"%\"},")
    f.write("{\"Clicks\":\""+str(clicksConversion)+" - "+str(int(clicksConversion/totalPosted*100))+"%\"},") #str(clicksConversion)+"-"+str(int(clicksConversion/totalPosted*100))+"%
    f.write("{\"Purchase\":\""+str(purchaseConversion)+" - "+str(int(purchaseConversion/totalPosted*100))+"%\"},") #+str(purchaseConversion)+"-"+str(int(purchaseConversion/totalPosted*100))+
    f.write("{\"ZeroHits\":\""+str(zeroHitQueries)+"\"}") #"+str(zeroHitQueries)+"
    f.write("]},") #closting for summary-->total_posted

    f.write("{\"Total_From_All_Queries\":[") #opening for summary-->total_posted
    f.write("{\"Total_Queries\":"+str(int(totalQueries))+"},")
    f.write("{\"Total_Posted\":\""+str(totalPosted)+" - "+str(int(totalPosted/totalQueries*100))+"%\"},")
    f.write("{\"Views\":\""+str(viewsConversion)+" - "+str(int(viewsConversion/totalQueries*100))+"%\"},")
    f.write("{\"Clicks\":\""+str(clicksConversion)+" - "+str(int(clicksConversion/totalQueries*100))+"%\"},")
    f.write("{\"Purchase\":\""+str(purchaseConversion)+" - "+str(int(purchaseConversion/totalQueries*100))+"%\"},")
    f.write("{\"ZeroHits\":\""+str(zeroHitQueries)+"\"}") #"+str(zeroHitQueries)+"
    f.write("]}") #closting for summary-->total_posted

    f.write("],") #closing for results_summary object
    f.write("\"TotalQueryDistributions\":"+jsonifyDictionar(totalPostDistribution)+",")
    f.write("\"channel_posts_distribution\":"+jsonifyDictionar(channelDistribution)+",")
    f.write("\"segment_product_distribution\":"+jsonifyDictionar(segmentProductDistribution)+",")
    f.write("\"posted_title_distribution\":"+jsonifyDictionar(postedTitles)+",")
    f.write("\"total_view_per_title\":"+jsonifyDictionar(totalPostViewsPerTitle)+",")
    f.write("\"total_purchased_per_title\":"+jsonifyDictionar(totalPostedPurchaseEvents)+",")
    f.write("\"total_clicks_implicitEvents_per_title\":"+jsonifyDictionar(totalPostedImplicitEvents)+",")
    f.write("\"experiment_properties\":[") #opening for experiment results object
    f.write("{\"search_terms\":"+jsonifyDictionar(searchQs)+"},")
    f.write("{\"viewed_title_w_ommitted_queryids\":"+jsonifyDictionar(ommit_queryids_titles)+"},")
    f.write("{\"views_portion_set\":"+jsonifyDictionar(views_portion)+"},")
    f.write("{\"randomize\":"+jsonifyDictionar(randomize)+"},")
    f.write("{\"searchType_TAPS\":"+jsonifyDictionar(searchType)+"},")
    f.write("{\"useCase\":"+jsonifyDictionar(useCase)+"},")
    f.write("{\"property_path\":"+jsonifyDictionar(prop_path)+"}")
    f.write("],") # closing of experiment properties object
    f.write("\"activatedRules\":"+jsonifyDictionar(activatedCounts)+",")
    f.write("\"activatedCampaigns\":"+jsonifyDictionar(campaignTreatments)+",")
    f.write("\"experimentTreatments\":"+jsonifyDictionar(experimentTreatments)+",")
    f.write("\"total non-campaign queries\":"+jsonifyDictionar(nonPromotedQueryCounts)+",")
    f.write("\"total promoted campaign queries\":"+jsonifyDictionar(promotedQueryCounts)+"")
    f.write("}") #closing for json object
    f.close()
    print("\n\tSimulation Ran For "+str(minutes)+" Minutes - Results Recorded To "+target_out)
    findDuplicateTreatmentUsers(experimentTreatments)
    print("total non-campaign queries "+str(nonPromotedQueryCounts))
    print("total promoted campaign queries "+str(promotedQueryCounts))

runSimulation()
printStatistics(allActivatedRules)
sys.exit(0)